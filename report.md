# Behaviorial Cloning Project

Author: Diogo Pontes

Email: dpontes11@gmail.com

## Objective of the Project

To clone driver behaviour using what was learned in Deep Learning. A model will be trained, validated and tested using Keras. The model will output a steering angle to an autonomous vehicle.

## Files Submitted & Code Quality

### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project include the following files:

- model.py containing the script to create and train the model
- drive.py for driving the car in autonomous mode
- model.h5 containing a trained convolution neural network
- report.md summarizing the results
- run1.mp4 is the video of resulting successful run of circuit 1

### 2. Submission includes functional code

Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing:

```sh
python drive.py model.h5
```

Some observations:

- The car is able to run around the track without touching the margins
- The car runs quite slowly. I'm not sure as to why this is

### 3. Submission code is usable and readable

The _model.py_ file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works

## Model Architecture and Traning Strategy

### 1. An appropriate model architecture has been deployed

The model uses the same network as described in the NVIDIA paper.

The model includes ReLU layers to introduce nonlinearity in every convolutional and fully connected layer, and the data is normalized in the model using Keras lambda layer (line 101).

Cropping is used to eliminate the sky and the bottom of the image and 24 pixels each side to compensate the black areas due to the image generation procedures.

### 2. Attempts to reduce overfitting in the model

The model contains dropout layers to reduce overfitting between the fully connected layers.

The model was trained and validated on different data sets 80/20 percent on the total data. The sets were generated by the _buildDirectory(filelist)_ procedure.

No big overfitting was observed. probably 15 epochs was too low to show it.

### 3. Model parameter tuning

The model uses an Adam Optimizer, so the learning rate was not tuned manually.

When training in the first it was a bit of trying quite a few good runs in order for the model to be able to learn how to drive "properly".

### 4. Appropriate training data

The training data used was a pretty smooth run that actually showed suficient for the model to performed as observed in the _run1.mp4_ video.

## Model Architecture and Training Strategy

### 1. Solution Design Approach

The NVIDIA model seemed to be a good place to start.

### 2. Final Model Architecture

The model architecture consisted of a convolution NN with the following layers and layer sizes:

| Layer         		|     Description	        					|
|:---------------------:|:---------------------------------------------:|
| Input         		| 160x320x3 image   							|
| lambda                | Normalization image values                          |
| Cropping           	| 60,25 top/bottom, 24,24 left/right         	|
| Convolution 5x5     	| 2x2 stride, valid padding, relu  24 filters   |
| Convolution 5x5     	| 2x2 stride, valid padding, relu  36 filters   |
| Convolution 5x5     	| 2x2 stride, valid padding, relu  48 filters   |
| Convolution 3x3     	| 1x1 stride, valid padding, relu  64 filters   |
| Convolution 3x3     	| 1x1 stride, valid padding, relu  64 filters   |
| Flatten				| 								                |
| Fully connected       | Outputs 100, relu                             |
| dropout               | Keep Probability 0.5                          |
| Fully connected       | Outputs 50, relu                              |
| Fully connected       | Outputs 10, relu                              |
| dropout               | Keep Probability 0.5                          |
| Fully connected       | Outputs 1, relu                               |

The cropping layer is used to remove sky and bottom car parts and also to remove the sides which are rendered unusable due to the data augmentation procedure.

Due to this generation process images should be cropped in the sides to remove the generated black areas.

### 3. Creation of the Training Set & Training Process

Images from the 3 cameras were used. left and right camera steering values were corrected by a **correction**. As we will see this value is the critical parameter of the model.

In a real environment it may be computed from geometric constraints but here I should estimate it. Beginning by a suggestion that it was about 0.2.

First I recorded 1 turn of first track. It was clearly insufficient at places like
the bridge.

Two more turns were enough to get a robust enough model. No need to record special
cases as recovering.

All data was split 80/20 between train and validation sets.


